services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: claims-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: claims-kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://claims-kafka:9092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

  kafdrop:
    image: obsidiandynamics/kafdrop:4.0.2
    container_name: claims-kafdrop
    depends_on:
      - kafka
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKERCONNECT: "kafka:9092"
      SERVER_PORT: 9000

  postgres:
    image: postgres:16
    container_name: claims-postgres
    restart: always
    environment:
      POSTGRES_DB: claimsdb
      POSTGRES_USER: claims_user
      POSTGRES_PASSWORD: claims_password
    ports:
      - "5432:5432"
    volumes:
      - claims-postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U claims_user -d claimsdb"]
      interval: 10s
      timeout: 5s
      retries: 5

  prometheus:
    image: prom/prometheus:latest
    container_name: claims-prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
    ports:
      - "9091:9090"  # au√üen 9091, innen 9090

  grafana:
    image: grafana/grafana:latest
    container_name: claims-grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      # optional, nur Kosmetik:
      GF_USERS_DEFAULT_THEME: dark
    volumes:
      # Dashboard-JSONs aus dem Host
      - ./grafana/dashboards:/var/lib/grafana/dashboards
      # Provisioning-Konfigurationen (z.B. claims-dashboard.yml)
      - ./grafana/provisioning:/etc/grafana/provisioning

  node-exporter:
    image: prom/node-exporter:v1.8.2
    container_name: claims-node-exporter
    restart: unless-stopped
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'


  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: claims-cadvisor
    restart: unless-stopped
    ports:
      - "8082:8080"   # UI auf http://localhost:8082
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro

  k6-breakpoint:
    image: grafana/k6:latest
    container_name: claims-k6-breakpoint
    profiles: ["loadtest"]
    depends_on:
      - prometheus
    volumes:
      - ./loadtests:/scripts
      - ./claim-service/claim-service/src/main/proto:/proto
    entrypoint: [ "k6", "run", "/scripts/claim_breakpoint.js" ]

  k6-constant:
    image: grafana/k6:latest
    container_name: claims-k6-constant
    profiles: ["loadtest"]
    depends_on:
      - prometheus
    volumes:
      - ./loadtests:/scripts
      - ./claim-service/claim-service/src/main/proto:/proto
    entrypoint: [ "k6", "run", "/scripts/claim_constant_load.js" ]




volumes:
  claims-postgres-data:
